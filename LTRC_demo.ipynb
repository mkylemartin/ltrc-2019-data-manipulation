{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome\n",
    "\n",
    "This notebook was used for the LTRC 2019 demo. \n",
    "\n",
    "It loads and analyzes various datasets and then creates a number of statistical models to see which ones perform better. Unsurprisingly, with more data, the combined model performed best. An additional model excluding two variables (age and L1) was output to be used publicly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AES_function import analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_CFE = pd.read_excel('Kyle\\'s model/CFE_dataset.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Analyze FCE Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the two essays into one and use the overall score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 0\n",
      "completed 100\n",
      "completed 200\n",
      "completed 300\n",
      "completed 400\n",
      "completed 500\n",
      "completed 600\n",
      "completed 700\n",
      "completed 800\n",
      "completed 900\n",
      "completed 1000\n",
      "completed 1100\n",
      "completed 1200\n"
     ]
    }
   ],
   "source": [
    "fce_analyzed = []\n",
    "for i in range(0, len(raw_CFE)):\n",
    "    row = raw_CFE.iloc[i]\n",
    "    \n",
    "    data = {}\n",
    "#     data['essay1'] = row['1-just_text']\n",
    "#     data['essay2'] = row['2-just_text']\n",
    "    \n",
    "    essays = row['1-just_text'] + ' ' + row['2-just_text']\n",
    "    essays = re.sub('\\n', ' ', essays)  # replace whitespace\n",
    "\n",
    "#     data['combined_essays'] = essays\n",
    "    data['age_code'] = row['age_code']\n",
    "    data['language_id'] = row['language_id']\n",
    "    data['score'] = row['score']  # Score is out of 40\n",
    "    # Combine the row data with the feature_set data\n",
    "    combined = {**data, **analyze(essays)}\n",
    "\n",
    "    fce_analyzed.append(combined)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('completed', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Analyce ELC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "elc_df = pd.read_excel('kyle_certified_clean_train_set.xlsx', sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3616 to analyze!\n",
      "completed 0\n",
      "completed 100\n",
      "completed 200\n",
      "completed 300\n",
      "completed 400\n",
      "completed 500\n",
      "completed 600\n",
      "completed 700\n",
      "completed 800\n",
      "completed 900\n",
      "completed 1000\n",
      "completed 1100\n",
      "completed 1200\n",
      "completed 1300\n",
      "completed 1400\n",
      "completed 1500\n",
      "completed 1600\n",
      "completed 1700\n",
      "completed 1800\n",
      "completed 1900\n",
      "completed 2000\n",
      "completed 2100\n",
      "completed 2200\n",
      "completed 2300\n",
      "completed 2400\n",
      "completed 2500\n",
      "completed 2600\n",
      "completed 2700\n",
      "completed 2800\n",
      "completed 2900\n",
      "completed 3000\n",
      "completed 3100\n",
      "completed 3200\n",
      "completed 3300\n",
      "completed 3400\n",
      "completed 3500\n",
      "completed 3600\n"
     ]
    }
   ],
   "source": [
    "elc_analyzed = []\n",
    "print(f'{len(elc_df)} to analyze!')\n",
    "\n",
    "for i in range(0, len(elc_df)):\n",
    "    row = elc_df.iloc[i]\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    data['age_code'] = row['age_code']\n",
    "    data['language_id'] = row['language_id']\n",
    "    data['score'] = row['score']\n",
    "    combined = {**data, **analyze(row['essay'])}\n",
    "\n",
    "    elc_analyzed.append(combined)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('completed', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble, metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(df, score_column, output='model'):\n",
    "    \"\"\" `df` is the dataframe to be passed in\n",
    "        `scores` is the name of the column you want to analyze as scores\n",
    "        \n",
    "        returns a model and prints general statistical data at time of calling\"\"\"\n",
    "    \n",
    "    scores = df[score_column]\n",
    "    # make sure to only select the score column\n",
    "    \n",
    "    voulu = ['age_code', 'ari', 'avg_len_word', 'cli', 'conjunctions', 'cttr',\n",
    "             'dcrs', 'determiners', 'dw', 'english_usage', 'fkg', 'fre',\n",
    "             'function_ttr', 'gf', 'grammar_chk', 'language_id', 'lwf',\n",
    "             'n_bigram_lemma_types', 'n_bigram_lemmas', 'n_trigram_lemma_types',\n",
    "             'n_trigram_lemmas', 'ncontent_words', 'nfunction_words', 'nlemma_types',\n",
    "             'nlemmas', 'noun_ttr', 'num_tokens', 'num_types', 'pct_rel_trigrams',\n",
    "             'pct_transitions', 'rank_avg', 'rank_total', 's1', 's1a', 's1b', 's1c',\n",
    "             's2', 's2a', 's2b', 's2c', 's3', 's3a', 's3b', 's3c', 's4', 's4a',\n",
    "             's4b', 's4c', 'sent_density', 'spelling_perc', 'ttr']\n",
    "    \n",
    "    features = df[voulu]\n",
    "    # make sure to only select the feature columns\n",
    "    \n",
    "    # there are a few responses with weird scores\n",
    "    bad = []\n",
    "    for i, value in enumerate(scores.values):\n",
    "        try:\n",
    "            float(value)\n",
    "        except (ValueError, TypeError):\n",
    "            bad.append(i)\n",
    "    \n",
    "    features = features.drop(index=bad)\n",
    "    \n",
    "    scores = scores.drop(index=bad)\n",
    "    \n",
    "    \n",
    "    # create numpy arrays of values\n",
    "    X = features.values\n",
    "    y = scores.values\n",
    "    \n",
    "    kfolds = KFold(n_splits=12, shuffle=True)\n",
    "    \n",
    "    for train_index, test_index in kfolds.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = ensemble.GradientBoostingRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=4,\n",
    "        min_samples_leaf=9,\n",
    "        max_features=0.3,\n",
    "        loss='lad',\n",
    "        random_state=0)\n",
    "    \n",
    "    # prints a bunch of data about the model\n",
    "    print('`model.fit` results:')\n",
    "    print(model.fit(X, y))\n",
    "    \n",
    "    train_error = mean_absolute_error(y_train, model.predict(X_train))\n",
    "    test_error = mean_absolute_error(y_test, model.predict(X_test))\n",
    "\n",
    "    print('Mean Absolute Error:')\n",
    "    print('Train error:', train_error, sep='\\t')\n",
    "    print('Test error:', test_error, sep='\\t')\n",
    "    print()\n",
    "    \n",
    "    r2_train = metrics.r2_score(y_train, model.predict(X_train))\n",
    "    r2_test = metrics.r2_score(y_test, model.predict(X_test))\n",
    "\n",
    "    # r2 is the proportion of the variance in the scores that is predictable from the features\n",
    "    print('r2 scores of both train/test:')\n",
    "    print('r2_train:', r2_train, sep='\\t')\n",
    "    print('r2_test:', r2_test, sep='\\t')\n",
    "    print()\n",
    "    \n",
    "    if output == 'model':\n",
    "        return model\n",
    "    elif output == 'pickle':\n",
    "        name = input(\"INPUT `{name}.pkl` and press [ENTER]\")\n",
    "        joblib.dump(model, name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fce_dataframe = pd.DataFrame(fce_analyzed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLC Data Test/Train Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`model.fit` results:\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.02, loss='lad', max_depth=4, max_features=0.3,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=9,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=500, n_iter_no_change=None, presort='auto',\n",
      "             random_state=0, subsample=1.0, tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Mean Absolute Error:\n",
      "Train error:\t2.2845839249248145\n",
      "Test error:\t2.8586375357788802\n",
      "\n",
      "r2 scores of both train/test:\n",
      "r2_train:\t0.6351009332989177\n",
      "r2_test:\t0.5993034515626057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fce_gbr_model = build_model(fce_dataframe, 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info(model, column_names):\n",
    "#     print(len(model.feature_importances_))\n",
    "    results = sorted(list(zip(column_names,\n",
    "                          model.feature_importances_)),\n",
    "                     key=lambda x: x[1], reverse=True)\n",
    "    for k, v in results:\n",
    "        print(k, v, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "elc_df = pd.DataFrame(elc_analyzed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "elc_actual_scores = elc_df['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "voulu = ['age_code', 'ari', 'avg_len_word', 'cli', 'conjunctions', 'cttr',\n",
    "       'dcrs', 'determiners', 'dw', 'english_usage', 'fkg', 'fre',\n",
    "       'function_ttr', 'gf', 'grammar_chk', 'language_id', 'lwf',\n",
    "       'n_bigram_lemma_types', 'n_bigram_lemmas', 'n_trigram_lemma_types',\n",
    "       'n_trigram_lemmas', 'ncontent_words', 'nfunction_words', 'nlemma_types',\n",
    "       'nlemmas', 'noun_ttr', 'num_tokens', 'num_types', 'pct_rel_trigrams',\n",
    "       'pct_transitions', 'rank_avg', 'rank_total', 's1', 's1a', 's1b', 's1c',\n",
    "       's2', 's2a', 's2b', 's2c', 's3', 's3a', 's3b', 's3c', 's4', 's4a',\n",
    "       's4b', 's4c', 'sent_density', 'spelling_perc', 'ttr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "elc_features = elc_df[voulu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted ELC scores \n",
    "\n",
    "(predicted as iLexiR, which are out of 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.89785554, 34.76541296, 33.77101161, ..., 15.18876042,\n",
       "       15.48425496, 29.32852078])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fce_gbr_model.predict(elc_features.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len 3616\n"
     ]
    }
   ],
   "source": [
    "elc_pred_scores = fce_gbr_model.predict(elc_features.values)\n",
    "print('len', len(pred_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import regression metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error,\\\n",
    "median_absolute_error, r2_score, mean_squared_log_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = {}\n",
    "feedback['explained_variance_score'] = explained_variance_score(elc_actual_scores.values * 8, elc_pred_scores)\n",
    "feedback['mean_absolute_error'] = mean_absolute_error(elc_actual_scores.values * 8, elc_pred_scores)\n",
    "feedback['mean_squared_error'] = mean_squared_error(elc_actual_scores.values * 8, elc_pred_scores)\n",
    "feedback['mean_squared_log_error'] = mean_squared_log_error(elc_actual_scores.values * 8, elc_pred_scores)\n",
    "feedback['median_absolute_error'] = median_absolute_error(elc_actual_scores.values * 8, elc_pred_scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Feedback Metrics\n",
    "\n",
    "(How well does the CFE `iLexIR` data predict the BYU data?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score  0.10406482613832191\n",
      "mean_absolute_error       121.27637931234035\n",
      "mean_squared_error        16979.74118458355\n",
      "mean_squared_log_error    3.2062134157092905\n",
      "median_absolute_error     122.61835951115071\n"
     ]
    }
   ],
   "source": [
    "for k, v in feedback.items():\n",
    "    print('{:25} {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of actual scores to predicted scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70191912],\n",
       "       [0.70191912, 1.        ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrcoef(elc_actual_scores.values, pred_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Data for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_scores</th>\n",
       "      <th>pred_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3616.000000</td>\n",
       "      <td>3616.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.968833</td>\n",
       "      <td>22.613985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.342027</td>\n",
       "      <td>3.983842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.485268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.400000</td>\n",
       "      <td>19.709239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.150000</td>\n",
       "      <td>22.387664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>22.100000</td>\n",
       "      <td>25.316536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>35.786499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       actual_scores  pred_scores\n",
       "count    3616.000000  3616.000000\n",
       "mean       17.968833    22.613985\n",
       "std         6.342027     3.983842\n",
       "min         0.000000    14.485268\n",
       "25%        14.400000    19.709239\n",
       "50%        18.150000    22.387664\n",
       "75%        22.100000    25.316536\n",
       "max        40.000000    35.786499"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'actual_scores': elc_actual_scores.values, 'pred_scores': elc_pred_scores}\n",
    "scores = pd.DataFrame(d)\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_scores</th>\n",
       "      <th>pred_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_scores</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.701919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_scores</th>\n",
       "      <td>0.701919</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               actual_scores  pred_scores\n",
       "actual_scores       1.000000     0.701919\n",
       "pred_scores         0.701919     1.000000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.to_csv('output_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a `LinearRegression` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our two datasets as dataframes are stored as:\n",
    "- `elc_df` and\n",
    "- `fce_dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(df, score_column, output='model'):\n",
    "    \"\"\" `df` is the dataframe to be passed in\n",
    "        `scores` is the name of the column you want to analyze as scores\n",
    "        \n",
    "        returns a model and prints general statistical data at time of calling\"\"\"\n",
    "    \n",
    "    scores = df[score_column]\n",
    "    # make sure to only select the score column\n",
    "    \n",
    "    voulu = ['age_code', 'ari', 'avg_len_word', 'cli', 'conjunctions', 'cttr',\n",
    "             'dcrs', 'determiners', 'dw', 'english_usage', 'fkg', 'fre',\n",
    "             'function_ttr', 'gf', 'grammar_chk', 'language_id', 'lwf',\n",
    "             'n_bigram_lemma_types', 'n_bigram_lemmas', 'n_trigram_lemma_types',\n",
    "             'n_trigram_lemmas', 'ncontent_words', 'nfunction_words', 'nlemma_types',\n",
    "             'nlemmas', 'noun_ttr', 'num_tokens', 'num_types', 'pct_rel_trigrams',\n",
    "             'pct_transitions', 'rank_avg', 'rank_total', 's1', 's1a', 's1b', 's1c',\n",
    "             's2', 's2a', 's2b', 's2c', 's3', 's3a', 's3b', 's3c', 's4', 's4a',\n",
    "             's4b', 's4c', 'sent_density', 'spelling_perc', 'ttr']\n",
    "    \n",
    "    features = df[voulu]  # means `wanted` in french\n",
    "    # make sure to only select the feature columns\n",
    "    \n",
    "    # there are a few responses with weird scores\n",
    "    bad = []\n",
    "    for i, value in enumerate(scores.values):\n",
    "        try:\n",
    "            float(value)\n",
    "        except (ValueError, TypeError):\n",
    "            bad.append(i)\n",
    "    \n",
    "    features = features.drop(index=bad)\n",
    "    \n",
    "    scores = scores.drop(index=bad)\n",
    "    \n",
    "    \n",
    "    # create numpy arrays of values\n",
    "    X = features.values\n",
    "    y = scores.values\n",
    "    \n",
    "    kfolds = KFold(n_splits=12, shuffle=True)\n",
    "    \n",
    "    for train_index, test_index in kfolds.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print('`model.fit` results:')\n",
    "    # prints a bunch of data about the model\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    print(model.fit(X, y))\n",
    "    \n",
    "    train_error = mean_absolute_error(y_train, model.predict(X_train))\n",
    "    test_error = mean_absolute_error(y_test, model.predict(X_test))\n",
    "\n",
    "    print('Mean Absolute Error:')\n",
    "    print('Train error:', train_error, sep='\\t')\n",
    "    print('Test error:', test_error, sep='\\t')\n",
    "    print()\n",
    "    \n",
    "    r2_train = metrics.r2_score(y_train, model.predict(X_train))\n",
    "    r2_test = metrics.r2_score(y_test, model.predict(X_test))\n",
    "\n",
    "    # r2 is the proportion of the variance in the scores that is predictable from the features\n",
    "    print('r2 scores of both train/test:')\n",
    "    print('r2_train:', r2_train, sep='\\t')\n",
    "    print('r2_test:', r2_test, sep='\\t')\n",
    "    print()\n",
    "    \n",
    "    if output == 'model':\n",
    "        return model\n",
    "    elif output == 'pickle':\n",
    "        name = input(\"INPUT `{name}.pkl` and press [ENTER]\")\n",
    "        joblib.dump(model, name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCE data predicting ELC data (`LinearRegression`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`model.fit` results:\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False)\n",
      "Mean Absolute Error:\n",
      "Train error:\t3.262040211073515\n",
      "Test error:\t3.3490489823818477\n",
      "\n",
      "r2 scores of both train/test:\n",
      "r2_train:\t0.4244652793649637\n",
      "r2_test:\t0.3900985843158298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fce_linear = linear_model(fce_dataframe, 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24.112961  ,  27.18838204,  31.36013423, ..., -22.90866452,\n",
       "       -19.95565726,  28.45143507])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elc_values = elc_features\n",
    "elc_targets = elc_actual_scores\n",
    "\n",
    "fce_linear_preds = fce_linear.predict(elc_values)\n",
    "fce_linear_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.63506871],\n",
       "       [0.63506871, 1.        ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrcoef(elc_targets, fce_linear_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a combined model and test it\n",
    "\n",
    "combine the analyzed CFE and BYU_ELC data and build a few models with it (using kfold validation):\n",
    "- `GradientBoostingRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4853"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model = elc_analyzed + fce_analyzed\n",
    "len(combined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_analyzed = pd.DataFrame(combined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`model.fit` results:\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.02, loss='lad', max_depth=4, max_features=0.3,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=9,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=500, n_iter_no_change=None, presort='auto',\n",
      "             random_state=0, subsample=1.0, tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Mean Absolute Error:\n",
      "Train error:\t2.403447835908283\n",
      "Test error:\t2.5535642426314715\n",
      "\n",
      "r2 scores of both train/test:\n",
      "r2_train:\t0.8124452471700421\n",
      "r2_test:\t0.7852482046513279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comb_model = build_model(comb_analyzed, 'score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Model \n",
    "\n",
    "Using linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_build_model_without_age_and_lang_id(df, score_column, output='model'):\n",
    "    \"\"\" `df` is the dataframe to be passed in\n",
    "        `scores` is the name of the column you want to analyze as scores\n",
    "        \n",
    "        returns a model and prints general statistical data at time of calling\"\"\"\n",
    "    \n",
    "    scores = df[score_column]\n",
    "    # make sure to only select the score column\n",
    "    \n",
    "    voulu = ['ari', 'avg_len_word', 'cli', 'conjunctions', 'cttr',\n",
    "             'dcrs', 'determiners', 'dw', 'english_usage', 'fkg', 'fre',\n",
    "             'function_ttr', 'gf', 'grammar_chk', 'lwf',\n",
    "             'n_bigram_lemma_types', 'n_bigram_lemmas', 'n_trigram_lemma_types',\n",
    "             'n_trigram_lemmas', 'ncontent_words', 'nfunction_words', 'nlemma_types',\n",
    "             'nlemmas', 'noun_ttr', 'num_tokens', 'num_types', 'pct_rel_trigrams',\n",
    "             'pct_transitions', 'rank_avg', 'rank_total', 's1', 's1a', 's1b', 's1c',\n",
    "             's2', 's2a', 's2b', 's2c', 's3', 's3a', 's3b', 's3c', 's4', 's4a',\n",
    "             's4b', 's4c', 'sent_density', 'spelling_perc', 'ttr']\n",
    "    \n",
    "    features = df[voulu]\n",
    "    # make sure to only select the feature columns\n",
    "    \n",
    "    # there are a few responses with weird scores\n",
    "    bad = []\n",
    "    for i, value in enumerate(scores.values):\n",
    "        try:\n",
    "            float(value)\n",
    "        except (ValueError, TypeError):\n",
    "            bad.append(i)\n",
    "    \n",
    "    features = features.drop(index=bad)\n",
    "    \n",
    "    scores = scores.drop(index=bad)\n",
    "    \n",
    "    \n",
    "    # create numpy arrays of values\n",
    "    X = features.values\n",
    "    y = scores.values\n",
    "    \n",
    "    kfolds = KFold(n_splits=12, shuffle=True)\n",
    "    \n",
    "    for train_index, test_index in kfolds.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = ensemble.GradientBoostingRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=4,\n",
    "        min_samples_leaf=9,\n",
    "        max_features=0.3,\n",
    "        loss='ls',\n",
    "        random_state=0)\n",
    "    \n",
    "    # prints a bunch of data about the model\n",
    "    print('`model.fit` results:')\n",
    "    print(model.fit(X, y))\n",
    "    \n",
    "    train_error = mean_absolute_error(y_train, model.predict(X_train))\n",
    "    test_error = mean_absolute_error(y_test, model.predict(X_test))\n",
    "\n",
    "    print('Mean Absolute Error:')\n",
    "    print('Train error:', train_error, sep='\\t')\n",
    "    print('Test error:', test_error, sep='\\t')\n",
    "    print()\n",
    "    \n",
    "    r2_train = metrics.r2_score(y_train, model.predict(X_train))\n",
    "    r2_test = metrics.r2_score(y_test, model.predict(X_test))\n",
    "\n",
    "    # r2 is the proportion of the variance in the scores that is predictable from the features\n",
    "    print('r2 scores of both train/test:')\n",
    "    print('r2_train:', r2_train, sep='\\t')\n",
    "    print('r2_test:', r2_test, sep='\\t')\n",
    "    print()\n",
    "\n",
    "    output = {}\n",
    "    output['predictions'] = model.predict(X)\n",
    "    output['actual_scores'] = y\n",
    "    output['model'] = model\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`model.fit` results:\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.02, loss='ls', max_depth=4, max_features=0.3,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=9,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=500, n_iter_no_change=None, presort='auto',\n",
      "             random_state=0, subsample=1.0, tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Mean Absolute Error:\n",
      "Train error:\t2.379653312177093\n",
      "Test error:\t2.4853308176068967\n",
      "\n",
      "r2 scores of both train/test:\n",
      "r2_train:\t0.834073647432878\n",
      "r2_test:\t0.8002512288445411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_ls_model = combined_build_model_without_age_and_lang_id(comb_analyzed, 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('elc_clc_combined_LRTC_model_13_march.pkl', 'wb') as f:\n",
    "    pickle.dump(c_ls_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_build_model(df, score_column, output='model'):\n",
    "    \"\"\" `df` is the dataframe to be passed in\n",
    "        `scores` is the name of the column you want to analyze as scores\n",
    "        \n",
    "        returns a model and prints general statistical data at time of calling\"\"\"\n",
    "    \n",
    "    scores = df[score_column]\n",
    "    # make sure to only select the score column\n",
    "    \n",
    "    voulu = ['age_code', 'ari', 'avg_len_word', 'cli', 'conjunctions', 'cttr',\n",
    "             'dcrs', 'determiners', 'dw', 'english_usage', 'fkg', 'fre',\n",
    "             'function_ttr', 'gf', 'grammar_chk', 'language_id', 'lwf',\n",
    "             'n_bigram_lemma_types', 'n_bigram_lemmas', 'n_trigram_lemma_types',\n",
    "             'n_trigram_lemmas', 'ncontent_words', 'nfunction_words', 'nlemma_types',\n",
    "             'nlemmas', 'noun_ttr', 'num_tokens', 'num_types', 'pct_rel_trigrams',\n",
    "             'pct_transitions', 'rank_avg', 'rank_total', 's1', 's1a', 's1b', 's1c',\n",
    "             's2', 's2a', 's2b', 's2c', 's3', 's3a', 's3b', 's3c', 's4', 's4a',\n",
    "             's4b', 's4c', 'sent_density', 'spelling_perc', 'ttr']\n",
    "    \n",
    "    features = df[voulu]\n",
    "    # make sure to only select the feature columns\n",
    "    \n",
    "    # there are a few responses with weird scores\n",
    "    bad = []\n",
    "    for i, value in enumerate(scores.values):\n",
    "        try:\n",
    "            float(value)\n",
    "        except (ValueError, TypeError):\n",
    "            bad.append(i)\n",
    "    \n",
    "    features = features.drop(index=bad)\n",
    "    \n",
    "    scores = scores.drop(index=bad)\n",
    "    \n",
    "    \n",
    "    # create numpy arrays of values\n",
    "    X = features.values\n",
    "    y = scores.values\n",
    "    \n",
    "    kfolds = KFold(n_splits=12, shuffle=True)\n",
    "    \n",
    "    for train_index, test_index in kfolds.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = ensemble.GradientBoostingRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=4,\n",
    "        min_samples_leaf=9,\n",
    "        max_features=0.3,\n",
    "        loss='ls',\n",
    "        random_state=0)\n",
    "    \n",
    "    # prints a bunch of data about the model\n",
    "    print('`model.fit` results:')\n",
    "    print(model.fit(X, y))\n",
    "    \n",
    "    train_error = mean_absolute_error(y_train, model.predict(X_train))\n",
    "    test_error = mean_absolute_error(y_test, model.predict(X_test))\n",
    "\n",
    "    print('Mean Absolute Error:')\n",
    "    print('Train error:', train_error, sep='\\t')\n",
    "    print('Test error:', test_error, sep='\\t')\n",
    "    print()\n",
    "    \n",
    "    r2_train = metrics.r2_score(y_train, model.predict(X_train))\n",
    "    r2_test = metrics.r2_score(y_test, model.predict(X_test))\n",
    "\n",
    "    # r2 is the proportion of the variance in the scores that is predictable from the features\n",
    "    print('r2 scores of both train/test:')\n",
    "    print('r2_train:', r2_train, sep='\\t')\n",
    "    print('r2_test:', r2_test, sep='\\t')\n",
    "    print()\n",
    "\n",
    "    output = {}\n",
    "    output['predictions'] = model.predict(X)\n",
    "    output['actual_scores'] = y\n",
    "    output['model'] = model\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`model.fit` results:\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.02, loss='ls', max_depth=4, max_features=0.3,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=9,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=500, n_iter_no_change=None, presort='auto',\n",
      "             random_state=0, subsample=1.0, tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Mean Absolute Error:\n",
      "Train error:\t2.360482423992901\n",
      "Test error:\t2.32169958721344\n",
      "\n",
      "r2 scores of both train/test:\n",
      "r2_train:\t0.8341340712062068\n",
      "r2_test:\t0.8575492864872207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combined, least-squares model\n",
    "c_ls_model = combined_build_model(comb_analyzed, 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.91516276],\n",
       "       [0.91516276, 1.        ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrcoef(c_ls_model['predictions'], c_ls_model['actual_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_ls_actual_scores</th>\n",
       "      <th>c_ls_pred_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4853.000000</td>\n",
       "      <td>4853.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20.504492</td>\n",
       "      <td>20.504492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.501940</td>\n",
       "      <td>6.603465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.724722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.893082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20.100000</td>\n",
       "      <td>20.999084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25.400000</td>\n",
       "      <td>25.222517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>39.386296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       c_ls_actual_scores  c_ls_pred_scores\n",
       "count         4853.000000       4853.000000\n",
       "mean            20.504492         20.504492\n",
       "std              7.501940          6.603465\n",
       "min              0.000000          1.724722\n",
       "25%             15.000000         15.893082\n",
       "50%             20.100000         20.999084\n",
       "75%             25.400000         25.222517\n",
       "max             40.000000         39.386296"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'c_ls_actual_scores': c_ls_model['actual_scores'], \n",
    "     'c_ls_pred_scores': c_ls_model['predictions']}\n",
    "c_ls_scores = pd.DataFrame(d)\n",
    "c_ls_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ls_scores.to_csv('6mar_c_ls_model_output_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_set = pd.read_excel('demo_set.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "elc_demo = demo_set[demo_set['which'] == 'elc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "elc_demo_feats = []\n",
    "for i in range(0, len(elc_demo)):\n",
    "    row = elc_demo.iloc[i]\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    data['age_code'] = row['age_code']\n",
    "    data['language_id'] = row['language_id']\n",
    "    data['score'] = row['score']\n",
    "    combined = {**data, **analyze(row['essay'])}\n",
    "\n",
    "    elc_demo_feats.append(combined)\n",
    "\n",
    "elc_demo_df = pd.DataFrame(elc_demo_feats)\n",
    "wanted_features = ['age_code', 'ari', 'avg_len_word', 'cli', 'conjunctions', 'cttr',\n",
    "                   'dcrs', 'determiners', 'dw', 'english_usage', 'fkg', 'fre',\n",
    "                   'function_ttr', 'gf', 'grammar_chk', 'language_id', 'lwf',\n",
    "                   'n_bigram_lemma_types', 'n_bigram_lemmas', 'n_trigram_lemma_types',\n",
    "                   'n_trigram_lemmas', 'ncontent_words', 'nfunction_words', 'nlemma_types',\n",
    "                   'nlemmas', 'noun_ttr', 'num_tokens', 'num_types', 'pct_rel_trigrams',\n",
    "                   'pct_transitions', 'rank_avg', 'rank_total', 's1', 's1a', 's1b', 's1c',\n",
    "                   's2', 's2a', 's2b', 's2c', 's3', 's3a', 's3b', 's3c', 's4', 's4a',\n",
    "                   's4b', 's4c', 'sent_density', 'spelling_perc', 'ttr']\n",
    "\n",
    "elc_demo_features = elc_demo_df[wanted_features]\n",
    "elc_demo_scores = elc_demo_df['score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict\tactual\n",
      "9.95\t5.0\n",
      "3.77\t5.05\n",
      "16.48\t20.0\n",
      "15.93\t20.0\n",
      "29.91\t33.25\n",
      "31.83\t33.25\n"
     ]
    }
   ],
   "source": [
    "print('predict', 'actual', sep='\\t')\n",
    "for p, t in zip(c_ls_model['model'].predict(elc_demo_features), elc_demo_scores):\n",
    "    print(round(p, 2), t, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "clc_demo = demo_set[demo_set['which'] == 'clc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "clc_demo_feats = []\n",
    "for i in range(0, len(elc_demo)):\n",
    "    row = clc_demo.iloc[i]\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    data['age_code'] = row['age_code']\n",
    "    data['language_id'] = row['language_id']\n",
    "    data['score'] = row['score']\n",
    "    combined = {**data, **analyze(row['essay'])}\n",
    "\n",
    "    clc_demo_feats.append(combined)\n",
    "\n",
    "clc_demo_df = pd.DataFrame(clc_demo_feats)\n",
    "wanted_features = ['age_code', 'ari', 'avg_len_word', 'cli', 'conjunctions', 'cttr',\n",
    "                   'dcrs', 'determiners', 'dw', 'english_usage', 'fkg', 'fre',\n",
    "                   'function_ttr', 'gf', 'grammar_chk', 'language_id', 'lwf',\n",
    "                   'n_bigram_lemma_types', 'n_bigram_lemmas', 'n_trigram_lemma_types',\n",
    "                   'n_trigram_lemmas', 'ncontent_words', 'nfunction_words', 'nlemma_types',\n",
    "                   'nlemmas', 'noun_ttr', 'num_tokens', 'num_types', 'pct_rel_trigrams',\n",
    "                   'pct_transitions', 'rank_avg', 'rank_total', 's1', 's1a', 's1b', 's1c',\n",
    "                   's2', 's2a', 's2b', 's2c', 's3', 's3a', 's3b', 's3c', 's4', 's4a',\n",
    "                   's4b', 's4c', 'sent_density', 'spelling_perc', 'ttr']\n",
    "\n",
    "clc_demo_features = clc_demo_df[wanted_features]\n",
    "clc_demo_scores = clc_demo_df['score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict\tactual\n",
      "20.13\t9.0\n",
      "18.47\t11.0\n",
      "24.37\t20.0\n",
      "20.53\t21.0\n",
      "31.73\t37.0\n",
      "32.92\t37.0\n"
     ]
    }
   ],
   "source": [
    "print('predict', 'actual', sep='\\t')\n",
    "for p, t in zip(c_ls_model['model'].predict(clc_demo_features), clc_demo_scores):\n",
    "    print(round(p, 2), t, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
